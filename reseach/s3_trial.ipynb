{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import zipfile\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY=\"AKIAYLZZJ4DCYYGG6UFD\"\n",
    "AWS_SECRET_KEY=\"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\"\n",
    "BUCKET_NAME=\"waste-model\"\n",
    "# 575108931781.dkr.ecr.us-east-1.amazonaws.com/chicken\n",
    "\n",
    "def get_s3fs():\n",
    "  return s3fs.S3FileSystem(key=AWS_ACCESS_KEY, secret=AWS_SECRET_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "  # Zipfile hook to zip up model folders\n",
    "  length = len(path) # Doing this to get rid of parent folders\n",
    "  for root, _, files in os.walk(path):\n",
    "    folder = root[length:] # We don't need parent folders! Why in the world does zipfile zip the whole tree??\n",
    "    for file in files:\n",
    "      ziph.write(os.path.join(root, file), os.path.join(folder, file))\n",
    "\n",
    "\n",
    "def s3_save_model(weight_path, model_name, BUCKET_NAME):\n",
    "  # with tempfile.TemporaryDirectory() as tempdir:\n",
    "    # model.save(f\"{tempdir}/{model_name}\")\n",
    "    # Zip it up first\n",
    "    # zipf = zipfile.ZipFile(f\"{weight_path}.zip\", \"w\", zipfile.ZIP_STORED)\n",
    "    # zipdir(f\"{weight_path}\", zipf)\n",
    "    # zipf.close()\n",
    "  s3fs = get_s3fs()\n",
    "  s3fs.put(f\"{weight_path}\", f\"{BUCKET_NAME}/{model_name}\")\n",
    "  logging.info(f\"Saved zipped model at path s3://{BUCKET_NAME}/{model_name}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_get_model(model_name: str, BUCKET_NAME: str):\n",
    "#   with tempfile.TemporaryDirectory() as tempdir:\n",
    "    s3fs = get_s3fs()\n",
    "    # Fetch and save the zip file to the temporary directory\n",
    "    s3fs.get(f\"{BUCKET_NAME}/{model_name}\", f\"{model_name}.pt\")\n",
    "    # Extract the model zip file within the temporary directory\n",
    "    # with zipfile.ZipFile(f\"{tempdir}/{model_name}.zip\") as zip_ref:\n",
    "    #     zip_ref.extractall(f\"{tempdir}/{model_name}\")\n",
    "    # Load the keras model from the temporary directory\n",
    "    logging.info(f\"Downloaded model from {BUCKET_NAME} bucket and saved locally to {model_name}\")\n",
    "    return f\"{model_name}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_save_model('D:/Mohit/end-to-end-deployment/artifacts/model_trainer/best.pt', \"waste_detction_yolov5.pt\", BUCKET_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waste_detction_yolov5.pt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_get_model(\"waste_detction_yolov5\", BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
